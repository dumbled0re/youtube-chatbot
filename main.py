import json
import os

import streamlit as st
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import YoutubeLoader
from langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,
                               SystemMessagePromptTemplate)
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

functions = [
    {
        "name": "get_question",
        "description": "è³ªå•ã‚’æŠ½å‡ºã™ã‚‹",
        "parameters": {
            "type": "object",
            "properties": {
                "question": {
                    "type": "string",
                    "description": "è³ªå•äº‹é … e.g. å‹•ç”»å†…å®¹ã‚’è¦ç´„ã—ã¦",
                    # "description": "è³ªå•äº‹é …",
                },
            },
            "required": ["question"],
        },
    },
    {
        "name": "get_keyword",
        "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹",
        "parameters": {
            "type": "object",
            "properties": {
                "keyword": {
                    "type": "string",
                    "description": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ e.g. ã€‡ã€‡ã«ã¤ã„ã¦ã€ã€‡ã€‡ã«é–¢ã—ã¦",
                },
            },
            "required": ["keyword"],
        }
    },
    {
        "name": "get_index",
        "description": "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã‚’æŠ½å‡ºã™ã‚‹",
        "parameters": {
            "type": "object",
            "properties": {
                "index": {
                    "type": "string",
                    "description": "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå· e.g. 1",
                },
            },
            "required": ["index"],
        }
    },
]


def get_question(llm, message):
    chat_completion = llm.predict_messages(
        [HumanMessage(content=message)],
        functions=functions,
    )

    return chat_completion
    # return message.additional_kwargs


def get_keyword(llm, message):
    chat_completion = llm.predict_messages(
        [HumanMessage(content=message)],
        functions=functions,
    )

    return chat_completion


def get_index(llm, message):
    chat_completion = llm.predict_messages(
        [HumanMessage(content=message)],
        functions=functions,
    )

    return chat_completion


def main():
    llm = ChatOpenAI(openai_api_key=os.environ.get("OPENAI_API_KEY"),
                     model_name="gpt-3.5-turbo-16k",
                     temperature=0)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,  # chunkã®æ–‡å­—æ•°
        chunk_overlap=10,  # chunké–“ã®é‡è¤‡æ–‡å­—æ•°
    )

    # ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã®éƒ¨åˆ†
    st.set_page_config(
        page_title="Youtube chatbot",
        page_icon="ğŸ¤—"
    )
    st.header("Youtube chatbot ğŸ¤—")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if not st.session_state:
        st.session_state.messages = [
            SystemMessage(content="You are a helpful assistant.")
        ]

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
    # if url := st.text_input("Youtube URL: ", key="input"):
    url = st.text_input("Youtube URL: ", key="input")
    print(f"url: {url}")
    if url:
        with st.spinner("Fetching Content ..."):
            loader = YoutubeLoader.from_youtube_url(
                url,
                # add_video_info=True,  # ã‚¿ã‚¤ãƒˆãƒ«ã‚„å†ç”Ÿæ•°ã‚‚å–å¾—ã§ãã‚‹
                language=['ja']
            )
            document = loader.load()
            chunk_document = text_splitter.split_text(document[0].page_content)
            # TODO: ã“ã“ã§ãƒãƒ£ãƒ³ã‚¯æ•°åˆ†ã®ç•ªå·ã¨æ–‡ç« ã®ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã‚’ä½œæˆã—ãŸã‚‰ã„ã„ã‹ã‚‚
            # TODO: ãã‚Œã‚’ã†ã¾ã„ã“ã¨gptã«å…¥ã‚Œã¦ã€è³ªå•ã«ç­”ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
            num_list = [i for i in range(len(chunk_document))]
            chunk_dict = dict(zip(num_list, chunk_document))
            print(f"chunk_dict: {chunk_dict}")
            video_content = document[0].page_content
            # print(f"document: {document}\n")
            # print(f"chunk_document: {chunk_document}\n")
            # print(f"video_content: {video_content}")

    if user_input := st.chat_input("ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("ç”Ÿæˆä¸­..."):
            # response = llm(st.session_state.messages)
            response = get_question(llm, user_input)
            print(f"###### response.additional_kwargs: {response.additional_kwargs}")
            if response.additional_kwargs:
                if response.additional_kwargs["function_call"]["name"] == "get_question":
                    print(f"response: {response}")
                    print(
                        f"response.additional_kwargs: {response.additional_kwargs}"
                        )

                    question = json.loads(
                        response.additional_kwargs["function_call"]["arguments"]
                        ).get("question")
                    print(f"question: {question}")

                    # system_template = "ã‚ãªãŸã¯ã€è³ªå•è€…ã‹ã‚‰ã®è³ªå•ã‚’{language}ã§å›ç­”ã™ã‚‹AIã§ã™ã€‚"
                    system_template = "ã‚ãªãŸã¯ã€è³ªå•è€…ã‹ã‚‰ã®è³ªå•ã‚’å›ç­”ã™ã‚‹AIã§ã™ã€‚"
                    human_template = """
                        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«ã€Œ{question}ã€ã«ã¤ã„ã¦ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

                        {document}
                    """

                    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)
                    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

                    print(f"system_message_prompt: {system_message_prompt}")
                    print(f"human_message_prompt: {human_message_prompt}")

                    chat_prompt = ChatPromptTemplate.from_messages(
                        [system_message_prompt, human_message_prompt])
                    prompt_message_list = chat_prompt.format_prompt(
                        language="æ—¥æœ¬èª",
                        question=question,
                        document=video_content).to_messages()
                    second_response = llm(prompt_message_list)
                    print(f"second_response: {second_response}")

                    st.session_state.messages.append(
                        AIMessage(content=second_response.content))
                elif response.additional_kwargs["function_call"]["name"] == "get_keyword":
                    print("get_keywordãŒå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸã€‚")
                    keyword = json.loads(
                        response.additional_kwargs["function_call"]["arguments"]
                        ).get("keyword")
                    print(f"keyword: {keyword}")

                    system_template = "ã‚ãªãŸã¯ã€ãƒªã‚¹ãƒˆã®ä¸­ã«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿å–ã£ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã‚’å›ç­”ã™ã‚‹AIã§ã™ã€‚"
                    # system_template = """
                    #     ã‚ãªãŸã¯ã€Pythonã®ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªå†…ã«ã‚ã‚‹ã‚­ãƒ¼ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿å–ã£ã¦é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã‚’å›ç­”ã™ã‚‹AIã§ã™ã€‚
                    # """
                    human_template = """
                        ä»¥ä¸‹ã®ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªå†…ã«ã‚ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«{keyword}ã®èª¬æ˜ãŒã•ã‚Œã¦ã‚ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã¯ä½•ç•ªã§ã—ã‚‡ã†ã‹ã€‚

                        {chunk_dict}

                        å›ç­”å½¢å¼ã¯ã€Œ{keyword}ã®èª¬æ˜ã¯{index}ç•ªã§ã™ã€‚ã€ã¨ã—ã¦ãã ã•ã„ã€‚
                        ã‚‚ã—ã‚‚ã€{keyword}ã®èª¬æ˜ãŒãªã„å ´åˆã¯ã€Œ{keyword}ã®èª¬æ˜ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã€ã¨ã—ã¦ãã ã•ã„ã€‚
                    """

                    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)
                    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

                    print(f"system_message_prompt: {system_message_prompt}")
                    print(f"human_message_prompt: {human_message_prompt}")

                    chat_prompt = ChatPromptTemplate.from_messages(
                        [system_message_prompt, human_message_prompt])
                    prompt_message_list = chat_prompt.format_prompt(
                        keyword=keyword,
                        chunk_dict=chunk_dict,
                        index="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹").to_messages()
                    second_response = llm(prompt_message_list)
                    print(f"second_response: {second_response}")
                    print(f"type(second_response.content): {type(second_response.content)}")
                    third_response = get_index(llm, second_response.content)
                    if third_response.additional_kwargs["function_call"]["name"] == "get_index":
                        index = json.loads(third_response.additional_kwargs["function_call"]["arguments"]).get("index")
                        print(f"index: {index}")
                        ai_answer = f"{keyword}ã®èª¬æ˜ã¯{int(index)*1}åˆ†ãã‚‰ã„ã‹ã‚‰ã§ã™ã€‚"
                        st.session_state.messages.append(
                            AIMessage(content=ai_answer))
                    else:
                        st.session_state.messages.append(
                            AIMessage(content=second_response.content))
            else:
                print("function callingãŒå‘¼ã³å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                response = llm(st.session_state.messages)
                st.session_state.messages.append(
                    AIMessage(content=response.content))

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    messages = st.session_state.get('messages', [])
    print(f"messages: {messages}")
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message('assistant'):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message('user'):
                st.markdown(message.content)


if __name__ == '__main__':
    main()
